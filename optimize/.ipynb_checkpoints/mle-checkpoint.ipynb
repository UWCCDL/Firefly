{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d3e27-9237-4c48-a89b-af3f37d4f310",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "#from tqdm.notebook import tqdm\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpy import exp, sqrt, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236fad2-fe70-474d-b803-1391dcb0bd26",
   "metadata": {},
   "source": [
    "# Recovering Prior Knowledge with Maximum Likelihood Estimation\n",
    "\n",
    "Here is a demo code on how to recover prior knowledge with MLE. First, we need some \"ground truth\" data; in this case, we will use Ally's data, which is saved in a CSV file called `cleandata_MLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb407f9-b813-4d45-9eda-c6ed5e23736c",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleandata_MLE.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da307073-ca5f-43e7-bc44-43b77cc1cf0c",
   "metadata": {},
   "source": [
    "We are going to focus on one particular participant, 83670, and one less only, Asian Flags. So we are going to isolate this set of facts into a subset of data called `sdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d3743-0201-4f8a-a3d7-8b5a18864ecc",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sdata = data\n",
    "sdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a5817-b64d-4d31-85c9-c2e1d718864e",
   "metadata": {},
   "source": [
    "Let's see how many facts we have -- it should be about 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2b2a0-0226-4d3f-86e5-2b33f5420fe7",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "len(set(sdata.factId))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe88f54-6726-461a-b4b0-a4612f8885e1",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "To do MLE, we need a model that predicts the probability of observing a specific response and response times for every fact that is presented during a session. We will also need to include some constraints on the parameters of the mode.\n",
    "\n",
    "First, we need to estimate the time that has passed from the first time they have seen the flag (in Qualtrics) and the first time they answer. We need to because QUaltrics is the equivalent of a \"study\" trial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebd5fc-7608-4a87-968a-3e76591475b8",
   "metadata": {},
   "source": [
    "Qualtrics presentations lasted 5 mins 20 seconds. Thus, the average time that has passed since the presentation of an item is 1/2 pf that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e4a2e-96fb-4fd2-a588-0c563900c3bb",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "offset = (5*60 + 20 )/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b804de-66eb-49e7-b229-220e604dfe2b",
   "metadata": {},
   "source": [
    "Some parameters are kept constant at the group level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947a1ce-e695-401e-9a88-1232a5a4edf0",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "RT = -0.8   # Retrieval threshold\n",
    "TER = 0.3   # time for encoding and responding\n",
    "F = 1.0     # Latency factor\n",
    "C = 0.25    # Spacing weight\n",
    "S = 0.25    # Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae520e-89e6-4200-99f5-ee3d3e6b27c0",
   "metadata": {},
   "source": [
    "Now, let's define our model. Our model is given by two equations that determine the activation of a memory given a value of $\\alpha$ and teh times at which the traces $t_1, t_2, \\dots, t_N$ have been created:\n",
    "\n",
    "$A(m,t) = \\sum_i (t- t_i)^{-d(i)}$\n",
    "\n",
    "$d(i) = c \\times e^{-A(m,t)} + \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3655d-b4c4-4a83-a30c-4cf786e0b0af",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def calculate_activation(time, traces, alpha, c=C):\n",
    "    traces = [x for x in traces if x < time]\n",
    "    d = alpha\n",
    "    memory_odds = 0\n",
    "    for trace in traces:\n",
    "        trace_odds = (time - trace) ** -d\n",
    "        memory_odds += trace_odds\n",
    "        d = c * np.exp(-np.log(memory_odds)) + alpha\n",
    "    return np.log(memory_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f30d9-6f66-4f0c-a910-7a308660be6e",
   "metadata": {},
   "source": [
    "Now, we can define our likelihood functions. The two likelihood functions will give us the two probability density functions for responses and response times, given some individual parameters (activation, threshold, noise, TER, and F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1c2b0-76d6-419e-969c-9cbf449cbd24",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def resp_prob(observed, activation, threshold, noise):\n",
    "    prob = 1 / (1 + np.exp(-(activation - threshold)/noise))\n",
    "    if observed == 0:\n",
    "        return 1 - prob\n",
    "    else:\n",
    "        return prob\n",
    "\n",
    "\n",
    "def rt_prob(observed, activation, threshold, noise, f, ter):    \n",
    "    alpha = exp(-activation) * f\n",
    "    beta = sqrt(3)/(pi*noise)\n",
    "    t = observed - ter\n",
    "    p =  ((beta/alpha) * (t/alpha)**(beta -1)) / (1 + (t/alpha)**beta)**2\n",
    "    if p is np.nan or p is np.inf:\n",
    "        return 0\n",
    "    else:\n",
    "        return p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af8b3b-b8df-4a81-84b6-5d1820b95d5a",
   "metadata": {},
   "source": [
    "And now, let's look at the examples of two PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49bb83-b7e5-47b9-b13b-1b725868a679",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "accuracies = np.array((0, 1))\n",
    "p = [resp_prob(x, 0, -0.8, 0.3) for x in accuracies]\n",
    "print(p)\n",
    "plt.bar([\"Incorrect\", \"Correct\"], p)\n",
    "plt.title(\"Probability Density for Response Accuracies (RT = %.1f; $s$ = %.2f)\" % (-0.8, 0.3))\n",
    "plt.xlabel(\"Response\")\n",
    "plt.ylim((0,1))\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "t = np.linspace(0.1, 10, 1000)\n",
    "prt = rt_prob(t, activation=0, threshold=-0.8, noise=0.3, f=1, ter=0.3)\n",
    "plt.plot(t, prt)\n",
    "plt.title(\"Probability Density for Response Times (RT = %.1f; $s$ = %.2f, $F$ = %.1f, $T_{ER}$ = %dms)\" % (-0.8, 0.3, 1.0, 300))\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlim((0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f98a5-1be6-4390-adee-3d989cd83792",
   "metadata": {},
   "source": [
    "# Applying MLE \n",
    "\n",
    "Here is a walk-thorugh of how MLE works. We start with the Asian Flags data for our test subject, saved in `sdata`. Of this data, we are going to focus only on the presentations pertaining a specific fact, and save this data in `sfdata`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b38764-9aad-404b-9c19-dff15784f472",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sfdata = sdata[sdata.factId == 337301]\n",
    "sfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8346d-0210-46f4-9e46-f394e116ec05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Log-likelihood function\n",
    "\n",
    "Now, we create a _log-likelihood_ function. The function loops for every presentation in `sfdata` (every presentation of a specific fact) and  calculates the probabilities of observing each of the corresponding responses and response times, given a specific set of parameter values. The probabilities are then log-transformed and summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef01c9a-dc3e-47fa-86c5-f09b5af47601",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loglikelihood(data, sof, rt, ter, f, noise, blc):\n",
    "    \"\"\"Log likelihood\"\"\"\n",
    "    traces = [0.0001]\n",
    "    start = list(set(data.start))[0]\n",
    "    LL = 0.0\n",
    "    for i in range(data.shape[0]):\n",
    "        event = data.iloc[i, :]\n",
    "        time = (event[\"presentationStartTime\"] - start)/1000 + offset\n",
    "        accuracy = event[\"correct\"] \n",
    "        resp_time = event[\"reactionTime\"] \n",
    "        # Expected activation\n",
    "        activation = blc + calculate_activation(time, traces, sof)\n",
    "        \n",
    "        # Log-likelihoods\n",
    "        LL += np.log(resp_prob(accuracy, activation, rt, noise))\n",
    "        LL += np.log(rt_prob(resp_time, activation, rt, noise, f, ter))\n",
    "                        \n",
    "        # Add new encoding to traces\n",
    "        traces.append(time)\n",
    "    \n",
    "    return LL\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad6eed-fc72-4478-be54-6def2233e68e",
   "metadata": {
    "tags": []
   },
   "source": [
    "For example, here is the log-likelihood of the observed results in `sfdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d14328-6507-4fb6-b171-b759b235a889",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "DATA = data\n",
    "\n",
    "loglikelihood(DATA, sof=0.3, rt=0.8, ter=0.3, f=1.0, noise=0.2, blc=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33362e2-598e-4e81-ae47-7442c8239bec",
   "metadata": {},
   "source": [
    "We are interested in using log-likelihood to identify prior knowledge, which is the represented by the base-level constant `BLC` of each fact.  For a given set of presentations of that fact, we are going to find the value of BLC that _maximizes_ the log-likelihood (given the rest of the params).\n",
    "\n",
    "To do so, we need to using SciPy _minimization_ techniques. Minimization techniques are machine learning tools that find the oarameters that minimize a given function. called the _Loss_ function. In this case, the loss function is a function of one parameter (the value of BLC) that computes the inverse of the log-likelihood (minimize loss = maximize likelihood).\n",
    "\n",
    "Because the loss function needs to know a participant's SOF, we need to store that in a global variable (it can be changed for other participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d35b1-6a97-4599-9712-9bdeffe4c2b5",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "SOF = 0.3\n",
    "def loss(params):\n",
    "    global DATA\n",
    "    blc = params[0]\n",
    "    data = DATA\n",
    "    return -loglikelihood(data, sof=SOF, rt=RT, ter=TER, f=F, noise=S, blc=blc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a6903-c30e-456e-96e4-cf329cbca427",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an example: The value of BLC that minimizes the current data saved in `sfdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3e03b-afd4-4983-b08b-18af9a4573ef",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "DATA = sfdata\n",
    "minimize(loss, x0 = [0],\n",
    "         method=\"Powell\", \n",
    "         tol=0.0000001, \n",
    "         bounds=[(-3, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e72c34-811d-4731-8219-85c01dcd05b1",
   "metadata": {},
   "source": [
    "We can repeat the procedure for every fact in `sfdata`, and computer their associated BLC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5677e4d-0be7-4b37-a91a-f4d1445e76fe",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "estimates = []\n",
    "\n",
    "for fact in set(sdata.answer):\n",
    "    DATA = sdata[sdata.answer == fact]\n",
    "    blc = minimize(loss, x0 = [0],\n",
    "                   method=\"Powell\", \n",
    "                   tol=0.0000001, \n",
    "                   bounds=[(-10, 10)])\n",
    "    estimates.append((fact, float(blc.x)))\n",
    "estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51572f8-d4b5-42d6-bd7a-892708bca6a9",
   "metadata": {
    "tags": [],
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "facts = [x[0] for x in estimates]\n",
    "blcs = [x[1] for x in estimates]\n",
    "plt.bar(facts, blcs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758824e0-b3b7-498a-a063-7c397d667bcf",
   "metadata": {},
   "source": [
    "# Functions for Ally\n",
    "\n",
    "Here is a function that you can call anytime --- you just need to specify the subject number, the lesson (\"Asian Flags\" vs \"Caribbean Flags\") and your best estimate for that subject's SOF (ideally, your mean SOF for the \"unknown\" data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_prior_knowledge(subject, lesson, sof):\n",
    "    global DATA\n",
    "    global SOF\n",
    "    SOF = sof\n",
    "    sdata = DATA[DATA.userId == subject]  # extract all data for that subject\n",
    "    sldata = sdata[sdata.lessonTitle == lesson]  # extract all data for that lesson\n",
    "\n",
    "    estimates = []\n",
    "    print(set(sldata.answer))\n",
    "    for fact in set(sldata.answer):\n",
    "        print(fact)\n",
    "        DATA = sldata[sldata.answer == fact]  # The DATA global var is accessed by the MLE loss function\n",
    "        \n",
    "#        # Get the corresponding prior_knowledge value from sldata\n",
    "#        prior_knowledge = sldata[sldata.answer == fact]['prior_knowledge'].iloc[0]\n",
    "\n",
    "        blc = minimize(loss, x0=[0],\n",
    "                       method=\"Powell\",\n",
    "                       tol=0.0000001,\n",
    "                       bounds=[(-10, 10)]).x\n",
    "        \n",
    "        estimates.append((fact, float(blc), subject, lesson))\n",
    "    return pd.DataFrame(estimates, columns=(\"Fact\", \"BLC\", \"Subject\", \"Lesson\"))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d804ad337cccdf29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = None\n",
    "\n",
    "#for sub in set(data.userId):\n",
    "#    mydata = data[data.userId == sub] # Get all the data for that subject\n",
    "#    for lesson in set(data.lessonTitle):\n",
    "        #mysmalldata= mydata[mydata.lessonTitle == lesson]\n",
    "        #average_alpha = np.mean(mysmalldata.average_alpha)  \n",
    "#        partial = decode_prior_knowledge(userId, lessonTitle, alpha)\n",
    "#        if results is None:\n",
    "#            results = partial\n",
    "#        else:\n",
    "#            results = pd.concat([results, partial], ignore_index=True, axis=0)\n",
    "            \n",
    "for user_id in set(data.userId):\n",
    "    user_data = data[data['userId'] == user_id]  # Get all the data for that user\n",
    "    \n",
    "    for lesson_title in set(user_data['lessonTitle']):\n",
    "        lesson_data = user_data[user_data['lessonTitle'] == lesson_title]\n",
    "        partial = decode_prior_knowledge(user_id, lesson_title, 0.3)\n",
    "        \n",
    "        if results is None:\n",
    "            results = partial\n",
    "        else:\n",
    "            results = pd.concat([results, partial], ignore_index=True, axis=0)\n",
    "            \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e9e8743308f78b4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(set(data['userId']))\n",
    "\n",
    "for user_id in set(data['userId']):\n",
    "    user_data = data[data['userId'] == user_id]  # Get all the data for that user\n",
    "    \n",
    "    # Print unique 'lessonTitle' values for the current user to verify\n",
    "    print(set(user_data['lessonTitle']))\n",
    "    \n",
    "    for lesson_title in set(user_data['lessonTitle']):\n",
    "        lesson_data = user_data[user_data['lessonTitle'] == lesson_title]\n",
    "        partial = decode_prior_knowledge(user_id, lesson_title, 0.3)\n",
    "        \n",
    "        if results is None:\n",
    "            results = partial\n",
    "        else:\n",
    "            results = pd.concat([results, partial], ignore_index=True, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "294a006c814ab660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "results = decode_prior_knowledge(user_id, lesson_title, 0.3)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5d596e6d8e964fa9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results.to_csv(\"results_MLE.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c29c3cbc71059c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "custom_order = [\"Venza\", \"Veloster\", \"Altroz\", \"Chelsea\", \"XC40\", \"CT%20200h\", \"URX%20NEO\", \"Q8%20e-tron\", \"Cherokee\", \"Explorer\"]\n",
    "\n",
    "plt.bar(results['Fact'], results['BLC'], color='lightblue')  # Set the color to blue\n",
    "plt.xticks(results['Fact'], custom_order, rotation='vertical')  # Rotates x-axis labels and uses custom order\n",
    "plt.xlabel('Fact')\n",
    "plt.ylabel('BLC')\n",
    "plt.title('Bar Graph with Custom Ordered X-Axis Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.bar(results['Fact'], results['BLC'], color='blue')  # Set the color to blue\n",
    "plt.xticks(rotation='vertical')  # Rotates x-axis labels vertically\n",
    "plt.xlabel('Fact')\n",
    "plt.ylabel('BLC')\n",
    "plt.title('Bar Graph with Vertical X-Axis Labels')\n",
    "\n",
    "# Set y-axis limits from 0 to 1\n",
    "plt.ylim(1, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "716cf10456726ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b00763610d6effc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
